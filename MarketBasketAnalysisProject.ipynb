{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7536b5d5-abf2-4cd5-a1c3-37e0f7f0480f",
      "metadata": {
        "id": "7536b5d5-abf2-4cd5-a1c3-37e0f7f0480f"
      },
      "source": [
        "# DATASET PROJECT : Counting Frequent Actors Pair"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This jupyter notebook is the implementation part of a project taking place in the course of Algorithm for massive datasets conducted by D. Malchiodi In the Universit√† degli Studi di Milano Statale. This course aims to devellop method to deal with massive datasets. In this project, we will see an application of this course in the problem of counting frequent itemsets. You can find a the github repository https://github.com/theot-student/Dataset-Project.git"
      ],
      "metadata": {
        "id": "yjz8IjbYpTXY"
      },
      "id": "yjz8IjbYpTXY"
    },
    {
      "cell_type": "markdown",
      "id": "b25950b8-a9eb-48b6-8c16-df4d924bdab2",
      "metadata": {
        "id": "b25950b8-a9eb-48b6-8c16-df4d924bdab2"
      },
      "source": [
        "## 1) Loading and Pre processing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bfcc920c-4a5e-49cf-83f4-e6f834609a2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfcc920c-4a5e-49cf-83f4-e6f834609a2c",
        "outputId": "4bad907d-4ae7-4f72-cec1-47b4c8fb3104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/gsimonx37/letterboxd\n",
            "License(s): GPL-3.0\n",
            "Downloading actors.csv.zip to /content\n",
            " 99% 48.0M/48.6M [00:00<00:00, 110MB/s] \n",
            "100% 48.6M/48.6M [00:00<00:00, 102MB/s]\n"
          ]
        }
      ],
      "source": [
        "#don't forget to replace the xxxxx by your own kaggle id in order to load the datas\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"xxxxx\"\n",
        "os.environ['KAGGLE_KEY'] = \"xxxxx\"\n",
        "!kaggle datasets download -d gsimonx37/letterboxd -f actors.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3187d6b4-8e60-44bc-815c-ddff698b4937",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3187d6b4-8e60-44bc-815c-ddff698b4937",
        "outputId": "3ae75e20-0d24-4769-8baa-c5feebd3730c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  actors.csv.zip\n",
            "  inflating: actors.csv              \n"
          ]
        }
      ],
      "source": [
        "!unzip actors.csv.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26c9445d-2c42-49fa-8b2b-88f1823d6fbe",
      "metadata": {
        "id": "26c9445d-2c42-49fa-8b2b-88f1823d6fbe"
      },
      "source": [
        "We make the import needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "24cf7a7a-cc4e-4afd-a33b-a1729ecc723b",
      "metadata": {
        "id": "24cf7a7a-cc4e-4afd-a33b-a1729ecc723b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6146f719-0e78-471f-b7a9-be488fa0f0fa",
      "metadata": {
        "id": "6146f719-0e78-471f-b7a9-be488fa0f0fa"
      },
      "source": [
        "Data loading with panda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "92ef680d-e833-4fe5-857c-bc4c4ee1ee1f",
      "metadata": {
        "id": "92ef680d-e833-4fe5-857c-bc4c4ee1ee1f"
      },
      "outputs": [],
      "source": [
        "actors = pd.read_csv('actors.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "40fa2bf4-374d-4128-af48-09cbe871191b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40fa2bf4-374d-4128-af48-09cbe871191b",
        "outputId": "1cfaf338-8e05-4f21-fab2-c6ce0a50a040"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5523327"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "actors.id.size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc2fcffb-c5f9-4464-9ecb-21aa53600a74",
      "metadata": {
        "id": "bc2fcffb-c5f9-4464-9ecb-21aa53600a74"
      },
      "source": [
        "Data processing into batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "358fc485-f5b5-4d58-8d64-a2dadeb449f9",
      "metadata": {
        "id": "358fc485-f5b5-4d58-8d64-a2dadeb449f9"
      },
      "outputs": [],
      "source": [
        "#this is the size of the data we want to process. Change it to change the duration of processing\n",
        "data_size = actors.id.size // 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "691d40f4-ebdd-4127-bfe3-1692435438b8",
      "metadata": {
        "id": "691d40f4-ebdd-4127-bfe3-1692435438b8"
      },
      "outputs": [],
      "source": [
        "actorsBatches = actors[:data_size].groupby(by='id')['name'].apply(np.array).to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6c524fe5-aad3-4826-bc1e-d05f6de50f0f",
      "metadata": {
        "id": "6c524fe5-aad3-4826-bc1e-d05f6de50f0f"
      },
      "outputs": [],
      "source": [
        "nb_batches = len(actorsBatches)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4870da6-9bb9-4e60-b830-e44fa62b90bd",
      "metadata": {
        "id": "f4870da6-9bb9-4e60-b830-e44fa62b90bd"
      },
      "source": [
        "We need to recover the frequency of each actors and the name of every single actors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "58a3ec26-2964-497b-9a44-8f1a290428e9",
      "metadata": {
        "id": "58a3ec26-2964-497b-9a44-8f1a290428e9"
      },
      "outputs": [],
      "source": [
        "actorsN = actors[:data_size].name.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4e250a47-ee9f-4901-8152-919ac74bf36e",
      "metadata": {
        "id": "4e250a47-ee9f-4901-8152-919ac74bf36e"
      },
      "outputs": [],
      "source": [
        "actors_name, frequency = np.unique(actorsN, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8af6d6fa-ba49-4801-a17e-e9c8f717fe06",
      "metadata": {
        "id": "8af6d6fa-ba49-4801-a17e-e9c8f717fe06"
      },
      "outputs": [],
      "source": [
        "actors_name = actors_name[np.argsort(frequency)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f3178b5e-33e0-4957-ae29-4f2201e1d306",
      "metadata": {
        "id": "f3178b5e-33e0-4957-ae29-4f2201e1d306"
      },
      "outputs": [],
      "source": [
        "frequency = np.sort(frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c9d6bf3f-bbc4-4511-a821-f07aeb9e16a8",
      "metadata": {
        "scrolled": true,
        "id": "c9d6bf3f-bbc4-4511-a821-f07aeb9e16a8"
      },
      "outputs": [],
      "source": [
        "integer2actor = actors_name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33aba992-dec6-499f-9acb-e1c06426cd54",
      "metadata": {
        "id": "33aba992-dec6-499f-9acb-e1c06426cd54"
      },
      "source": [
        "we need to create a dict to representing actor into integer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "00dd26cd-47a6-48bb-960d-455c6229f30f",
      "metadata": {
        "id": "00dd26cd-47a6-48bb-960d-455c6229f30f"
      },
      "outputs": [],
      "source": [
        "actor2integer = defaultdict()\n",
        "i = 0\n",
        "for actor in actors_name :\n",
        "    actor2integer[actor] = i\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d72dab-a704-45ff-821e-4261a2da42d5",
      "metadata": {
        "id": "54d72dab-a704-45ff-821e-4261a2da42d5"
      },
      "source": [
        "and to replace the name by their integer representation in the batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ce2c7cdc-4423-4a82-be92-cf83b4d681b7",
      "metadata": {
        "id": "ce2c7cdc-4423-4a82-be92-cf83b4d681b7"
      },
      "outputs": [],
      "source": [
        "for i in range(len(actorsBatches)):\n",
        "    for j in range(len(actorsBatches[i])):\n",
        "        actorsBatches[i][j] = actor2integer[actorsBatches[i][j]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c20d92e-eb87-4c14-965c-eb698d071044",
      "metadata": {
        "id": "6c20d92e-eb87-4c14-965c-eb698d071044"
      },
      "source": [
        "and finally compute boolean frequency of actors and fix the treshold s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "dfbee869-e9b3-483c-9b15-ec59d626c6b7",
      "metadata": {
        "id": "dfbee869-e9b3-483c-9b15-ec59d626c6b7"
      },
      "outputs": [],
      "source": [
        "s = frequency[-len(frequency) // 100]\n",
        "bool_frequency = np.zeros(np.size(frequency), dtype=bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "63938039-8ba4-4ece-8af4-ba8d3c6d397f",
      "metadata": {
        "id": "63938039-8ba4-4ece-8af4-ba8d3c6d397f"
      },
      "outputs": [],
      "source": [
        "bool_frequency[-len(frequency) // 100:] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a8c25102-e536-4d60-ab6c-0e6c5042e706",
      "metadata": {
        "id": "a8c25102-e536-4d60-ab6c-0e6c5042e706"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "efb898d6-1298-4258-b813-729c91ada977",
      "metadata": {
        "id": "efb898d6-1298-4258-b813-729c91ada977"
      },
      "source": [
        "## 2) PCY algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cab1ed1-443a-4737-a48b-c5139e014e9e",
      "metadata": {
        "id": "7cab1ed1-443a-4737-a48b-c5139e014e9e"
      },
      "source": [
        "First, we need to create an hash function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "820554d7-7ee5-4c16-a8ca-4c3cbc27baad",
      "metadata": {
        "id": "820554d7-7ee5-4c16-a8ca-4c3cbc27baad"
      },
      "outputs": [],
      "source": [
        "nb_bucket = len(actorsBatches) // 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "59e6ccd2-a83b-4e83-aa0e-be01d59c5db3",
      "metadata": {
        "id": "59e6ccd2-a83b-4e83-aa0e-be01d59c5db3"
      },
      "outputs": [],
      "source": [
        "def hash_pair(element1, element2):\n",
        "    bucket = (hash(element1) + hash(element2)) % nb_bucket\n",
        "    return bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bbe34aee-c6f7-476c-83c6-8bd5cc3f9820",
      "metadata": {
        "id": "bbe34aee-c6f7-476c-83c6-8bd5cc3f9820"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b4050a30-aa5d-45da-a8ee-16c248ea3089",
      "metadata": {
        "id": "b4050a30-aa5d-45da-a8ee-16c248ea3089"
      },
      "source": [
        "We now can start by implementing the PCY algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "44f5e9a2-c44c-42aa-9de4-3511bdc2493d",
      "metadata": {
        "id": "44f5e9a2-c44c-42aa-9de4-3511bdc2493d"
      },
      "outputs": [],
      "source": [
        "def PCY(batches, single_frequency, nb_bucket, treshold):\n",
        "    bitmap = np.zeros(nb_bucket)\n",
        "    hash_counters = np.zeros(nb_bucket)\n",
        "\n",
        "    #first pass along batches\n",
        "    for batch in batches:\n",
        "        for i in range(len(batch)):\n",
        "            for j in range(i+1,len(batch)):\n",
        "                hash_counters[hash_pair(batch[i],batch[j])] += 1\n",
        "\n",
        "    #we create our bitmap of frequent buckets with our treshold (we can change the treshold considering the number of data that are processed)\n",
        "    bucket_treshold = s                                                        #used for small size of data\n",
        "    #bucket_treshold = np.sort(hash_counters)[int(-len(hash_counters)//2)]       #used for large size of data\n",
        "    bitmap = [x >= bucket_treshold for x in hash_counters]\n",
        "\n",
        "    #second pass along batches\n",
        "    counters = defaultdict()\n",
        "    for batch in batches:\n",
        "        for i in range(len(batch)):\n",
        "            for j in range(i+1,len(batch)):\n",
        "                if (bitmap[hash_pair(batch[i],batch[j])] and single_frequency[batch[i]] and single_frequency[batch[j]]):\n",
        "                    k = min(batch[i],batch[j])\n",
        "                    l = max(batch[i],batch[j])\n",
        "                    if (k,l) in counters:\n",
        "                        counters[(k,l)] += 1\n",
        "                    else:\n",
        "                        counters[(k,l)] = 1\n",
        "\n",
        "    #then we filter our candidate pairs with the treshold\n",
        "    for actors_pair, counter in counters.copy().items():\n",
        "        if counter < treshold:\n",
        "            counters.pop(actors_pair)\n",
        "\n",
        "    return hash_counters, bitmap, counters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1a7de8f4-b712-4e75-b1b0-a3ac93b25bd5",
      "metadata": {
        "id": "1a7de8f4-b712-4e75-b1b0-a3ac93b25bd5"
      },
      "outputs": [],
      "source": [
        "hash_counters_PCY, bitmap_PCY, counters_PCY = PCY(actorsBatches, bool_frequency, nb_bucket, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32996751-a3e9-4960-b8a2-3e9f1dcb96f1",
      "metadata": {
        "id": "32996751-a3e9-4960-b8a2-3e9f1dcb96f1"
      },
      "source": [
        "We finish by remake our pair in a readable way by translating our integer into the real names of the actors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "3d399b7a-02ff-4466-be7f-b6d412a67ce8",
      "metadata": {
        "id": "3d399b7a-02ff-4466-be7f-b6d412a67ce8"
      },
      "outputs": [],
      "source": [
        "final_counters_PCY = dict()\n",
        "for actors_pair, counter in counters_PCY.items():\n",
        "    final_counters_PCY[integer2actor[actors_pair[0]] + ' and ' + integer2actor[actors_pair[1]]] = counter\n",
        "sorted_counters_PCY = sorted(final_counters_PCY.items(), key=lambda x:x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "aa8de7cb-c65a-4d86-8239-69ccf805a2ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa8de7cb-c65a-4d86-8239-69ccf805a2ba",
        "outputId": "6389293b-97c2-4953-b6d9-efdf36450bd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Harold Miller and Bess Flowers', 67),\n",
              " ('Jeff Bennett and Frank Welker', 57),\n",
              " ('Grey DeLisle and Frank Welker', 53),\n",
              " ('Ikue Otani and Megumi Hayashibara', 53),\n",
              " ('Rob Paulsen and Jeff Bennett', 51),\n",
              " ('Harold Miller and Sam Harris', 49),\n",
              " ('Ikue Otani and Kappei Yamaguchi', 48),\n",
              " ('Sam Harris and Bess Flowers', 46),\n",
              " ('Bert Stevens and Bess Flowers', 43),\n",
              " ('Stan Laurel and Oliver Hardy', 42)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "sorted_counters_PCY[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7c60101-beef-4daa-82a3-13f51d40d4b6",
      "metadata": {
        "id": "c7c60101-beef-4daa-82a3-13f51d40d4b6"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4fc38379-bd84-468d-aedb-02d7bd66cfa9",
      "metadata": {
        "id": "4fc38379-bd84-468d-aedb-02d7bd66cfa9"
      },
      "source": [
        "## 3) SON algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79c9c2fc-2418-404b-b6fa-70f4855f6533",
      "metadata": {
        "id": "79c9c2fc-2418-404b-b6fa-70f4855f6533"
      },
      "source": [
        "We will see 2 implementations of SON algorithm. The first one with classical numpy vector and considering we can store our data in the main memory. The second one will use a map reduce approach using spark."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "934c7f33-1f73-46c7-8235-f5786324626c",
      "metadata": {
        "id": "934c7f33-1f73-46c7-8235-f5786324626c"
      },
      "source": [
        "### a) without map reduce"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62484078-fa22-4a39-93dc-e6000def59fe",
      "metadata": {
        "id": "62484078-fa22-4a39-93dc-e6000def59fe"
      },
      "source": [
        "The idea is to first implement our son algorithm on our data without using map reduce (as in the 2) part). First we need to fix p = the number of chunks we will use. And we fix n the number of basket in each chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "115b376e-7aaf-4a33-8b1e-1df75dc0b389",
      "metadata": {
        "id": "115b376e-7aaf-4a33-8b1e-1df75dc0b389"
      },
      "outputs": [],
      "source": [
        "n = 20000\n",
        "p = n / nb_batches"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF9na4SeG-_j",
        "outputId": "69944e7f-357a-400c-bd85-e1023f2ac8f9"
      },
      "id": "NF9na4SeG-_j",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.37997530160539567"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1/p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgcyCuxn_oEi",
        "outputId": "60efdd3d-8989-408b-d708-341555a5751e"
      },
      "id": "IgcyCuxn_oEi",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.63175"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proba = 1"
      ],
      "metadata": {
        "id": "25kAxt6n8zYb"
      },
      "id": "25kAxt6n8zYb",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_Random_Algo(chunk, single_frequency, proba, chunk_treshold):\n",
        "  counters = defaultdict()\n",
        "\n",
        "  #we sample the batch if needed\n",
        "  random_batches = chunk\n",
        "  if proba != 1:\n",
        "    random_batches = np.random.choice(chunk, size=int(len(chunk) * proba), replace=False)\n",
        "\n",
        "  #perform a double loop to recover the frequency of pairs\n",
        "  for batch in random_batches:\n",
        "     for i in range(len(batch)):\n",
        "          for j in range(i+1,len(batch)):\n",
        "              if  (single_frequency[batch[i]] and single_frequency[batch[j]]):\n",
        "                  k = min(batch[i],batch[j])\n",
        "                  l = max(batch[i],batch[j])\n",
        "                  if (k,l) in counters:\n",
        "                      counters[(k,l)] += 1\n",
        "                  else:\n",
        "                      counters[(k,l)] = 1\n",
        "\n",
        "  #then we filter our candidate pairs with the treshold\n",
        "  sample_treshold = chunk_treshold * proba\n",
        "\n",
        "  for actors_pair, counter in counters.copy().items():\n",
        "      if counter < sample_treshold:\n",
        "          counters.pop(actors_pair)\n",
        "\n",
        "  return set(counters.keys())"
      ],
      "metadata": {
        "id": "mo0I7Xp86Q6n"
      },
      "id": "mo0I7Xp86Q6n",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First version of SON algorithm"
      ],
      "metadata": {
        "id": "tqmXimwWath1"
      },
      "id": "tqmXimwWath1"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "a29fbdaf-9fbb-4d19-b912-2040f8282011",
      "metadata": {
        "id": "a29fbdaf-9fbb-4d19-b912-2040f8282011"
      },
      "outputs": [],
      "source": [
        "def SON(batches, single_frequency, treshold, n, p, proba):\n",
        "    candidates = set()\n",
        "    counters = dict()\n",
        "    #we will apply the simple_random_algorithm on all chunk of the complete data\n",
        "    chunk_treshold = p * s\n",
        "\n",
        "    for i in range(int(1//p)):\n",
        "        chunk = batches[i*n : (i+1)*n]\n",
        "        candidates = candidates.union(simple_Random_Algo(chunk, single_frequency, proba, chunk_treshold))\n",
        "    chunk = batches[int((1//p)) * n :]\n",
        "    candidates = candidates.union(simple_Random_Algo(chunk, single_frequency, proba, chunk_treshold))\n",
        "\n",
        "    print(len(candidates))\n",
        "\n",
        "    for pair in candidates:\n",
        "      counters[pair] = 0\n",
        "    for batch in batches:\n",
        "      for i in range(len(batch)):\n",
        "        for j in range(i+1,len(batch)):\n",
        "          k = min(batch[i],batch[j])\n",
        "          l = max(batch[i],batch[j])\n",
        "          if ((k,l) in candidates):\n",
        "            counters[(k,l)] += 1\n",
        "\n",
        "    #then we filter our candidate pairs with the treshold\n",
        "    for actors_pair, counter in counters.copy().items():\n",
        "        if counter < chunk_treshold:\n",
        "            counters.pop(actors_pair)\n",
        "\n",
        "    return counters"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second version with the more clever way to deal with the second scan"
      ],
      "metadata": {
        "id": "pq5TH0VWawjX"
      },
      "id": "pq5TH0VWawjX"
    },
    {
      "cell_type": "code",
      "source": [
        "def SON_efficient(batches, single_frequency, treshold, n, p, proba):\n",
        "    candidates = set()\n",
        "    counters = dict()\n",
        "    #we will apply the rsimple_random_algorithm on all chunk of the complete data\n",
        "    chunk_treshold = p * s\n",
        "\n",
        "    for i in range(int(1//p)):\n",
        "        chunk = batches[i*n : (i+1)*n]\n",
        "        candidates = candidates.union(simple_Random_Algo(chunk, single_frequency, proba, chunk_treshold))\n",
        "    chunk = batches[int((1//p)) * n :]\n",
        "    candidates = candidates.union(simple_Random_Algo(chunk, single_frequency, proba, chunk_treshold))\n",
        "\n",
        "    #check if the number of candidate is not to huge\n",
        "    print(len(candidates))\n",
        "\n",
        "    single_candidates = set()\n",
        "    for pair in candidates:\n",
        "      single_candidates.add(pair[0])\n",
        "      single_candidates.add(pair[1])\n",
        "\n",
        "    for pair in candidates:\n",
        "      counters[pair] = 0\n",
        "\n",
        "    #we remove the actors that are not in our candidate pairs\n",
        "    for batch in batches:\n",
        "      batch_set = set(batch)\n",
        "      for actor in batch.copy():\n",
        "        if not(actor in single_candidates):\n",
        "          batch_set.remove(actor)\n",
        "\n",
        "      if len(batch_set) > 1:\n",
        "        new_batch = list(batch_set)\n",
        "\n",
        "        for i in range(len(new_batch)):\n",
        "          for j in range(i+1,len(new_batch)):\n",
        "            k = min(new_batch[i],new_batch[j])\n",
        "            l = max(new_batch[i],new_batch[j])\n",
        "            if ((k,l) in candidates):\n",
        "              counters[(k,l)] += 1\n",
        "\n",
        "    #then we filter our candidate pairs with the treshold\n",
        "    for actors_pair, counter in counters.copy().items():\n",
        "        if counter < chunk_treshold:\n",
        "            counters.pop(actors_pair)\n",
        "\n",
        "    return counters"
      ],
      "metadata": {
        "id": "TFszlCMk7Jog"
      },
      "id": "TFszlCMk7Jog",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "4b198c7b-63d3-47a4-8e51-7edf79f9b170",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b198c7b-63d3-47a4-8e51-7edf79f9b170",
        "outputId": "8d0265c0-4b7e-47d7-c127-45ca9dab5702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "861\n"
          ]
        }
      ],
      "source": [
        "counters_SON = SON(actorsBatches, bool_frequency, s, n, p, proba)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counters_SON_efficient = SON_efficient(actorsBatches, bool_frequency, s, n, p, proba)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Symb97xA9BRl",
        "outputId": "68c6ac5a-2385-48db-e2e9-4286b9159229"
      },
      "id": "Symb97xA9BRl",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number printed during the SON algorithm is the number of candidate after the first pass over data. This is print in order to chekc if the number is not to huge to have a reasonable time of processing. This number can be changed by changing the number of chunk (or the number n of batches in each chunk here) or the treshold s."
      ],
      "metadata": {
        "id": "6X4wjibeql9c"
      },
      "id": "6X4wjibeql9c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output for non efficient SON algorithm"
      ],
      "metadata": {
        "id": "ceOWg0Kpq-OB"
      },
      "id": "ceOWg0Kpq-OB"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "3fc3d889-076a-40df-bdec-211beac0204c",
      "metadata": {
        "id": "3fc3d889-076a-40df-bdec-211beac0204c"
      },
      "outputs": [],
      "source": [
        "final_counters_SON = dict()\n",
        "for actors_pair, counter in counters_SON.items():\n",
        "    final_counters_SON[integer2actor[actors_pair[0]] + ' and ' + integer2actor[actors_pair[1]]] = counter\n",
        "sorted_counters_SON = sorted(final_counters_SON.items(), key=lambda x:x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "0276e1ca-5d82-411d-895c-01fb4a875fb2",
      "metadata": {
        "id": "0276e1ca-5d82-411d-895c-01fb4a875fb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa854c1b-0f25-4be9-e8e5-792796b2dd94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Harold Miller and Bess Flowers', 67),\n",
              " ('Jeff Bennett and Frank Welker', 57),\n",
              " ('Ikue Otani and Megumi Hayashibara', 53),\n",
              " ('Grey DeLisle and Frank Welker', 53),\n",
              " ('Rob Paulsen and Jeff Bennett', 51),\n",
              " ('Harold Miller and Sam Harris', 49),\n",
              " ('Ikue Otani and Kappei Yamaguchi', 48),\n",
              " ('Sam Harris and Bess Flowers', 46),\n",
              " ('Bert Stevens and Bess Flowers', 43),\n",
              " ('Stan Laurel and Oliver Hardy', 42),\n",
              " ('Megumi Hayashibara and Koichi Yamadera', 41),\n",
              " ('Jim Cummings and Frank Welker', 39),\n",
              " ('Edna Purviance and Charlie Chaplin', 39),\n",
              " ('Jim Cummings and Jeff Bennett', 37),\n",
              " ('Dee Bradley Baker and Grey DeLisle', 37),\n",
              " ('Rob Paulsen and Frank Welker', 37),\n",
              " ('Bert Stevens and Harold Miller', 36),\n",
              " ('Franklyn Farnum and Bess Flowers', 35),\n",
              " ('Kenichi Ogata and Kappei Yamaguchi', 35),\n",
              " ('Sherry Lynn and Mickie McGowan', 35)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "sorted_counters_SON[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output of efficient SON algorithm"
      ],
      "metadata": {
        "id": "RaPtBO1Zqh8v"
      },
      "id": "RaPtBO1Zqh8v"
    },
    {
      "cell_type": "code",
      "source": [
        "final_counters_SON_efficient = dict()\n",
        "for actors_pair, counter in counters_SON_efficient.items():\n",
        "    final_counters_SON_efficient[integer2actor[actors_pair[0]] + ' and ' + integer2actor[actors_pair[1]]] = counter\n",
        "sorted_counters_SON_efficient = sorted(final_counters_SON_efficient.items(), key=lambda x:x[1], reverse=True)"
      ],
      "metadata": {
        "id": "JOVJeFrTHh-w"
      },
      "id": "JOVJeFrTHh-w",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_counters_SON_efficient[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qpfi8lLBmvJ",
        "outputId": "f353887a-6a49-4fa5-e9ca-e30e9ab0f5f7"
      },
      "id": "8Qpfi8lLBmvJ",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Harold Miller and Bess Flowers', 67),\n",
              " ('Jeff Bennett and Frank Welker', 57),\n",
              " ('Ikue Otani and Megumi Hayashibara', 53),\n",
              " ('Grey DeLisle and Frank Welker', 53),\n",
              " ('Rob Paulsen and Jeff Bennett', 51),\n",
              " ('Harold Miller and Sam Harris', 49),\n",
              " ('Ikue Otani and Kappei Yamaguchi', 48),\n",
              " ('Sam Harris and Bess Flowers', 46),\n",
              " ('Bert Stevens and Bess Flowers', 43),\n",
              " ('Stan Laurel and Oliver Hardy', 42),\n",
              " ('Megumi Hayashibara and Koichi Yamadera', 41),\n",
              " ('Jim Cummings and Frank Welker', 39),\n",
              " ('Edna Purviance and Charlie Chaplin', 39),\n",
              " ('Jim Cummings and Jeff Bennett', 37),\n",
              " ('Dee Bradley Baker and Grey DeLisle', 37),\n",
              " ('Rob Paulsen and Frank Welker', 37),\n",
              " ('Bert Stevens and Harold Miller', 36),\n",
              " ('Franklyn Farnum and Bess Flowers', 35),\n",
              " ('Kenichi Ogata and Kappei Yamaguchi', 35),\n",
              " ('Sherry Lynn and Mickie McGowan', 35)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d8b4e56-79ab-48a3-9a57-7f83daf444a4",
      "metadata": {
        "id": "4d8b4e56-79ab-48a3-9a57-7f83daf444a4"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2fa776bd-ab8e-425f-89f3-73139282a63e",
      "metadata": {
        "id": "2fa776bd-ab8e-425f-89f3-73139282a63e"
      },
      "source": [
        "### b) using spark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a80fd3f-153a-47dd-9716-9aca19c91845",
      "metadata": {
        "id": "2a80fd3f-153a-47dd-9716-9aca19c91845"
      },
      "source": [
        "First we need to import spark. For this part I'm using the importation of spark which is present in the spark presentation jupyter notebook of the lecture of \"algorithm for massive dataset\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "5df9b364-40f1-4f32-be4b-e127a650d1aa",
      "metadata": {
        "id": "5df9b364-40f1-4f32-be4b-e127a650d1aa"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "64a5ad89-1202-42b5-b309-a0f45d80d77b",
      "metadata": {
        "id": "64a5ad89-1202-42b5-b309-a0f45d80d77b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init(\"spark-3.1.1-bin-hadoop3.2\")# SPARK_HOME\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "mhH2khdtgI0Y"
      },
      "id": "mhH2khdtgI0Y",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "type(spark)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import math"
      ],
      "metadata": {
        "id": "MD3vZibcgL2N"
      },
      "id": "MD3vZibcgL2N",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import collect_list\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import BooleanType"
      ],
      "metadata": {
        "id": "pQQD5_DQpMql"
      },
      "id": "pQQD5_DQpMql",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now come to the loading of our data"
      ],
      "metadata": {
        "id": "JaYa9R7sqBG9"
      },
      "id": "JaYa9R7sqBG9"
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.option(\"delimiter\", \";\").option(\"delimiter\", \",\").option(\"header\", True).csv('actors.csv')"
      ],
      "metadata": {
        "id": "t1Nfrfyxggp0"
      },
      "id": "t1Nfrfyxggp0",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before we introduce a maximum value of data to process"
      ],
      "metadata": {
        "id": "rBDhXvdkqeIy"
      },
      "id": "rBDhXvdkqeIy"
    },
    {
      "cell_type": "code",
      "source": [
        "sp_data_size = df.count() // 5"
      ],
      "metadata": {
        "id": "-VMnejpPql1a"
      },
      "id": "-VMnejpPql1a",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(sp_data_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQHCamreq8em",
        "outputId": "9ea53fa2-99b0-4f9c-e547-93921aca8261"
      },
      "id": "sQHCamreq8em",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "used_df = df.limit(sp_data_size)"
      ],
      "metadata": {
        "id": "cq60s2AWrEk0"
      },
      "id": "cq60s2AWrEk0",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_chunk = 3"
      ],
      "metadata": {
        "id": "3Imlf_tNsr_y"
      },
      "id": "3Imlf_tNsr_y",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_Frequent_Single(index, bool_spark_frequency):\n",
        "    return bool_spark_frequency[index]\n",
        "\n",
        "def first_map(chunk, s, bool_spark_frequency):\n",
        "    counters = defaultdict()\n",
        "    length = 0\n",
        "\n",
        "    #we first pass along our sample/chunk to recover our candidate pair regarding single frequency\n",
        "    for batch in chunk:\n",
        "      np_batch = batch[0]\n",
        "      length += 1\n",
        "      for i in range(len(np_batch)):\n",
        "        for j in range(i+1,len(np_batch)):\n",
        "          if (is_Frequent_Single(np_batch[i], bool_spark_frequency)and is_Frequent_Single(np_batch[j], bool_spark_frequency)):\n",
        "            k = min(np_batch[i],np_batch[j])\n",
        "            l = max(np_batch[i],np_batch[j])\n",
        "            if (k,l) in counters:\n",
        "              counters[(k,l)] += 1\n",
        "            else:\n",
        "              counters[(k,l)] = 1\n",
        "\n",
        "\n",
        "    #then we filter our candidate pairs with the treshold\n",
        "    sample_treshold = s\n",
        "    for actors_pair, counter in counters.copy().items():\n",
        "        if counter >= sample_treshold:\n",
        "            yield (1, [actors_pair])\n",
        "\n",
        "def first_reduce(pair1, pair2):\n",
        "    if (pair1 == pair2):\n",
        "      return pair1\n",
        "    else:\n",
        "      return pair1 + pair2\n",
        "\n",
        "def second_map(chunk, candidates, single_candidates):\n",
        "    counters = defaultdict()\n",
        "\n",
        "    for pair in candidates:\n",
        "      counters[pair] = 0\n",
        "\n",
        "    for batch in chunk:\n",
        "      batch_set = set(batch.batch)\n",
        "      for actor in batch.batch:\n",
        "        if not(actor in single_candidates):\n",
        "          batch_set.remove(actor)\n",
        "\n",
        "      if len(batch_set) > 1:\n",
        "        new_batch = list(batch_set)\n",
        "\n",
        "        for i in range(len(new_batch)):\n",
        "          for j in range(i+1,len(new_batch)):\n",
        "            k = min(new_batch[i],new_batch[j])\n",
        "            l = max(new_batch[i],new_batch[j])\n",
        "            if ((k,l) in candidates):\n",
        "              counters[(k,l)] += 1\n",
        "\n",
        "    for actors_pair, counter in counters.copy().items():\n",
        "      yield (actors_pair, counter)\n",
        "\n",
        "def second_reduce(counter1, counter2):\n",
        "    return counter1 + counter2\n",
        "\n",
        "def check_treshold(counter, s):\n",
        "    if counter >= s:\n",
        "      return counter\n",
        "\n",
        "class MapReduceSON():\n",
        "\n",
        "  def __init__(self, data_size, df, nb_chunk):\n",
        "    self.data_size = data_size\n",
        "    self.df = df\n",
        "    self.nb_chunk = nb_chunk\n",
        "\n",
        "  def data_process(self):\n",
        "    #first we find our single frequency for each actors\n",
        "    frequency_df = self.df.groupby(\"name\").count()\n",
        "    frequency_sorted_df = frequency_df.sort(\"count\")\n",
        "\n",
        "    #then we index our actors names to provide a map from actor names to integer\n",
        "    frequency_sorted_df = frequency_sorted_df.withColumn(\"index\", monotonically_increasing_id())\n",
        "\n",
        "    #we need to create a mapping from index to name (i'm not sure if it's reliable for DFS)\n",
        "    actor_map = np.array(frequency_sorted_df.select(\"name\").collect())\n",
        "    self.actor_map = actor_map\n",
        "\n",
        "    #and we use index in our dataframe\n",
        "    df_indexed = self.df.join(frequency_sorted_df.select(\"name\",\"index\"), on=\"name\", how=\"left\")\n",
        "    self.df_indexed = df_indexed\n",
        "\n",
        "    #we create our batches\n",
        "    batches_df = self.df_indexed.groupby(\"id\").agg(collect_list(\"index\").alias(\"batch\")).repartition(nb_chunk, \"batch\")\n",
        "    self.batches_df = batches_df\n",
        "\n",
        "    #we create our treshold keeping only 1% of most frequent single actors\n",
        "    s = frequency_sorted_df.select(\"count\").take(frequency_sorted_df.count() - (frequency_sorted_df.count() // 100))[-1]\n",
        "    s = s[\"count\"]\n",
        "    self.s = s\n",
        "\n",
        "    #then we create an array list to recover frequent single actors in boolean\n",
        "    aboveTresholdUDF = udf(lambda x: x >= s,BooleanType())\n",
        "    frequency_sorted_df = frequency_sorted_df.withColumn(\"bool\", aboveTresholdUDF(col(\"count\")))\n",
        "    bool_spark_frequency = np.array(frequency_sorted_df.select(\"index\",\"bool\").rdd.map(lambda x : [x[0], x[1]]).collect())\n",
        "    bool_spark_frequency = bool_spark_frequency[np.argsort(bool_spark_frequency[:,0]),1]\n",
        "    self.bool_spark_frequency = bool_spark_frequency\n",
        "\n",
        "    #we also need our number of batches\n",
        "    nb_batches = batches_df.count()\n",
        "    self.nb_batches = nb_batches\n",
        "\n",
        "\n",
        "  def map_reduce(self):\n",
        "    #we need to transform our class variables into local variables otherwise the map reduce doesn't work in spark (probably because of thread and because we get outside the class)\n",
        "    s = self.s\n",
        "    nb_batches = self.nb_batches\n",
        "    nb_chunk = self.nb_chunk\n",
        "    bool_spark_frequency = self.bool_spark_frequency\n",
        "    s_chunk = s / nb_chunk\n",
        "\n",
        "    #we create our rdd\n",
        "    rdd = self.batches_df.select(\"batch\").rdd\n",
        "\n",
        "    #the we do our first map reduce to get our candidates\n",
        "    candidates_MR = rdd.mapPartitions(lambda x : first_map(x, s_chunk, bool_spark_frequency)).reduceByKey(first_reduce).collect()\n",
        "    candidates = candidates_MR[0][1]\n",
        "    print(len(candidates))\n",
        "\n",
        "    #for the efficient version we create a set of single candidate\n",
        "    single_candidates = set()\n",
        "    for pair in candidates:\n",
        "      single_candidates.add(pair[0])\n",
        "      single_candidates.add(pair[1])\n",
        "\n",
        "    #and finally we proceed to our second map reduce to get the final counters\n",
        "    final_counters_MRSON = rdd.mapPartitions(lambda x : second_map(x, candidates, single_candidates)).reduceByKey(second_reduce).reduceByKey(lambda x : check_treshold(x, s)).collect()\n",
        "\n",
        "    #we rename our counters to make them readable\n",
        "    counters_named = []\n",
        "    for counter in final_counters_MRSON:\n",
        "      if counter[1] >= s:\n",
        "        counters_named += [[str(self.actor_map[counter[0][0], 0]) + \" and \" + str(self.actor_map[counter[0][1], 0]) + \" appear together in \" + str(counter[1]) + \" movies.\", str(counter[1])]]\n",
        "\n",
        "    return counters_named"
      ],
      "metadata": {
        "id": "0cbP_HRVLATW"
      },
      "id": "0cbP_HRVLATW",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SON = MapReduceSON(sp_data_size, used_df, nb_chunk)\n",
        "SON.data_process()"
      ],
      "metadata": {
        "id": "B7G3EEWjN3pA"
      },
      "id": "B7G3EEWjN3pA",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_counters_MRSON = SON.map_reduce()"
      ],
      "metadata": {
        "id": "f8xxwITw-qkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d76c8a4-be7b-44be-9ee9-c7350635bb48"
      },
      "id": "f8xxwITw-qkz",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output of spark SON algorithm"
      ],
      "metadata": {
        "id": "5lSZz7sPrXSU"
      },
      "id": "5lSZz7sPrXSU"
    },
    {
      "cell_type": "code",
      "source": [
        "np.flip(np.array(final_counters_MRSON)[np.argsort(np.array(final_counters_MRSON)[:, 1], 0),0])[:20]"
      ],
      "metadata": {
        "id": "uRbpf40JQEzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "672482a5-5b1f-4e8c-b3d3-8c223e948477"
      },
      "id": "uRbpf40JQEzx",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Harold Miller and Bess Flowers appear together in 67 movies.',\n",
              "       'Jeff Bennett and Frank Welker appear together in 57 movies.',\n",
              "       'Ikue Otani and Megumi Hayashibara appear together in 53 movies.',\n",
              "       'Grey DeLisle and Frank Welker appear together in 53 movies.',\n",
              "       'Rob Paulsen and Jeff Bennett appear together in 51 movies.',\n",
              "       'Harold Miller and Sam Harris appear together in 49 movies.',\n",
              "       'Ikue Otani and Kappei Yamaguchi appear together in 48 movies.',\n",
              "       'Sam Harris and Bess Flowers appear together in 46 movies.',\n",
              "       'Bert Stevens and Bess Flowers appear together in 43 movies.',\n",
              "       'Stan Laurel and Oliver Hardy appear together in 42 movies.',\n",
              "       'Megumi Hayashibara and Koichi Yamadera appear together in 41 movies.',\n",
              "       'Jim Cummings and Frank Welker appear together in 39 movies.',\n",
              "       'Edna Purviance and Charlie Chaplin appear together in 39 movies.',\n",
              "       'Jim Cummings and Jeff Bennett appear together in 37 movies.',\n",
              "       'Dee Bradley Baker and Grey DeLisle appear together in 37 movies.',\n",
              "       'Rob Paulsen and Frank Welker appear together in 37 movies.',\n",
              "       'Bert Stevens and Harold Miller appear together in 36 movies.',\n",
              "       'Kenichi Ogata and Kappei Yamaguchi appear together in 35 movies.',\n",
              "       'Franklyn Farnum and Bess Flowers appear together in 35 movies.',\n",
              "       'Unsho Ishizuka and Megumi Hayashibara appear together in 35 movies.'],\n",
              "      dtype='<U76')"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RlrHhkJQnV-6"
      },
      "id": "RlrHhkJQnV-6",
      "execution_count": 60,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}